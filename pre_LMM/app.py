import streamlit as st
import os
import openai
import requests
import tempfile
import torch
from PIL import Image
from transformers import ViTFeatureExtractor, ViTForImageClassification
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain_community.document_loaders import YoutubeLoader
from langchain_community.callbacks.manager import get_openai_callback
from bs4 import BeautifulSoup
import speech_recognition as sr
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.chains import RetrievalQA
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from dotenv import load_dotenv
from pdf2image import convert_from_path
import pytesseract
import platform
from pdf2image import convert_from_path



# **ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒªãƒƒãƒˆã®ãƒšãƒ¼ã‚¸è¨­å®šã‚’æœ€åˆã«é…ç½®**
st.set_page_config(page_title="AI Multi-Tool", page_icon="ğŸ¤–")

# **ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿**
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if openai_api_key:
    print(f"âœ… APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™: {openai_api_key[:5]}...********")
else:
    print("ğŸš¨ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")

st.sidebar.text(f"API Key: {'âœ… è¨­å®šæ¸ˆã¿' if openai_api_key else 'âŒ æœªè¨­å®š'}")

# **Qdrantã®è¨­å®š**
QDRANT_PATH = "./local_qdrant"
COLLECTION_NAME = "pdf_collection"

# **ãƒ¢ãƒ‡ãƒ«é¸æŠ**
def select_model():
    if "model_name" not in st.session_state:
        st.session_state.model_name = "gpt-3.5-turbo"
        st.session_state.temperature = 0.0

    with st.sidebar:
        if "model_selection" not in st.session_state:
            model = st.radio("Choose a model:", ("GPT-3.5", "GPT-4"), key="model_selection")
            st.session_state.model_name = "gpt-3.5-turbo" if model == "GPT-3.5" else "gpt-4"

        if "temperature_selection" not in st.session_state:
            st.session_state.temperature = st.slider(
                "Temperature:", 0.0, 2.0, 0.0, 0.01, key="temperature_selection"
            )

    return ChatOpenAI(
        temperature=st.session_state.temperature,
        model_name=st.session_state.model_name,
        openai_api_key=openai.api_key
    )


# **Chat Bot**
def chat_bot(llm):
    st.header("ChatGPT ğŸ¤—")
    if "messages" not in st.session_state:
        st.session_state["messages"] = [SystemMessage(content="You are a helpful assistant.")]
    if user_input := st.chat_input("Ask me anything!"):
        st.session_state["messages"].append(HumanMessage(content=user_input))
        with st.spinner("ChatGPT is typing..."):
            response = llm(st.session_state["messages"])
        st.session_state["messages"].append(AIMessage(content=response.content))
    for message in st.session_state["messages"]:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)

# **ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¦ç´„**
def website_summarizer(llm):
    st.header("Website Summarizer ğŸŒ")
    url = st.text_input("Enter a website URL:")
    if url:
        with st.spinner("Fetching content..."):
            try:
                response = requests.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                content = soup.body.get_text() if soup.body else "No content available."
            except requests.RequestException as e:
                st.error(f"Failed to fetch the website: {e}")
                return
        prompt = f"Summarize the following text:\n{content[:1000]}"
        with st.spinner("Summarizing..."):
            response = llm([HumanMessage(content=prompt)])
        st.markdown("## Summary")
        st.write(response.content)

# **YouTubeè¦ç´„**
def youtube_summarizer(llm):
    st.header("YouTube Summarizer ğŸ¥")
    url = st.text_input("Enter a YouTube URL:")
    if url:
        with st.spinner("Fetching video transcript..."):
            try:
                loader = YoutubeLoader.from_youtube_url(url, add_video_info=True, language='ja')
                docs = loader.load()
            except Exception as e:
                st.error(f"Failed to fetch YouTube transcript: {e}")
                return
        prompt_template = "Summarize the following YouTube video transcript:\n{text}"
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["text"])
        chain = load_summarize_chain(llm, chain_type="stuff", verbose=True, prompt=PROMPT)
        response = chain.run(input_documents=docs)
        st.markdown("## Summary")
        st.write(response)

def image_recognition():
    st.header("Image Recognition & Summary ğŸ–¼ï¸")
    uploaded_file = st.file_uploader("Upload an image...", type=["jpg", "jpeg", "png"])
    
    if uploaded_file:
        image = Image.open(uploaded_file)
        st.image(image, caption="Uploaded Image", use_column_width=True)

        with st.spinner("Analyzing image..."):
            # ViTã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
            model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")
            inputs = feature_extractor(images=image, return_tensors="pt")
            outputs = model(**inputs)

            # ã‚¯ãƒ©ã‚¹äºˆæ¸¬
            predicted_class_idx = outputs.logits.argmax(-1).item()
            label_map = model.config.id2label  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ—ã‚’å–å¾—
            predicted_label = label_map.get(predicted_class_idx, "Unknown")

        st.markdown(f"### Prediction: {predicted_label} (Class {predicted_class_idx})")

        # AIã«ã‚ˆã‚‹èª¬æ˜ç”Ÿæˆ
        llm = select_model()
        prompt = f"This image appears to be a {predicted_label}. Provide a concise and informative description of such an object."
        with st.spinner("Generating description..."):
            response = llm([HumanMessage(content=prompt)])

        st.markdown("## Summary")
        st.write(response.content)


# **éŸ³å£°èªè­˜**
def speech_recognition():
    st.header("Speech Recognition ğŸ™ï¸")
    uploaded_audio = st.file_uploader("Upload an audio file...", type=["wav", "mp3", "m4a"])
    if uploaded_audio:
        st.success("Audio file uploaded successfully!")

# **ç”»åƒç”Ÿæˆ**
def image_generation():
    st.header("Image Generation ğŸ¨")
    prompt = st.text_input("Enter a prompt for image generation:")
    if prompt:
        with st.spinner("Generating image..."):
            response = openai.images.generate(
                model="dall-e-3",
                prompt=prompt,
                n=1,
                size="1024x1024"
            )
            image_url = response.data[0].url
            st.image(image_url, caption="Generated Image", use_column_width=True)

# **PDFãƒ†ã‚­ã‚¹ãƒˆå–å¾—**
def get_pdf_text():
    uploaded_file = st.file_uploader("Upload your PDF hereğŸ˜‡", type='pdf')

    if uploaded_file:
        pdf_reader = PdfReader(uploaded_file)

        # **é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**
        text = '\n\n'.join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])

        # **OCRå‡¦ç†ã®å¿…è¦æ€§ã‚’ç¢ºèª**
        if not text.strip():
            st.warning("No selectable text found. Using OCR for text extraction...")

            # **ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜**
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
                temp_pdf.write(uploaded_file.read())
                temp_pdf_path = temp_pdf.name  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

            # **Windowsã®Popplerãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆå¿…è¦ãªã‚‰ï¼‰**
            poppler_path = None
            if platform.system() == "Windows":
                poppler_path = r"C:\Program Files\poppler-23.01.0\bin"  # é©å®œå¤‰æ›´

            # **PDFã‚’ç”»åƒã«å¤‰æ›**
            images = convert_from_path(temp_pdf_path, dpi=300, poppler_path=poppler_path)

            # **OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**
            text = "\n\n".join([pytesseract.image_to_string(img) for img in images])

            # **ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤**
            os.remove(temp_pdf_path)

        # **OCRã§ã‚‚ãƒ†ã‚­ã‚¹ãƒˆãŒå–ã‚Œãªã‹ã£ãŸå ´åˆ**
        if not text.strip():
            st.error("Could not extract text from the PDF. It might be encrypted or unreadable.")
            return None

        # **ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²**
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            model_name="text-davinci-003",
            chunk_size=250,
            chunk_overlap=0,
        )
        return text_splitter.split_text(text)

    return None


# **Qdrantã®ãƒ­ãƒ¼ãƒ‰**
def load_qdrant():
    client = QdrantClient(path=QDRANT_PATH)
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]
    
    if COLLECTION_NAME not in collection_names:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )
    
    return Qdrant(
        client=client,
        collection_name=COLLECTION_NAME, 
        embeddings=OpenAIEmbeddings()
    )

# **PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆ**
def pdf_upload_and_build_vector_db():
    st.title("PDF Upload & Vector Search ğŸ“„")
    pdf_text = get_pdf_text()
    if pdf_text:
        with st.spinner("Loading PDF ..."):
            qdrant = load_qdrant()
            qdrant.add_texts(pdf_text)
        st.success("PDF successfully processed and indexed!")

def ask_my_pdf():
    st.title("Ask My PDF ğŸ“œ")

    # Qdrant ã®ãƒ­ãƒ¼ãƒ‰
    qdrant = load_qdrant()
    
    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèª
    existing_docs = qdrant.similarity_search("test", k=1)
    if not existing_docs:
        st.warning("âš ï¸ No indexed data found. Please upload and process a PDF first.")
        return

    # è³ªå•ã‚’å—ã‘ä»˜ã‘ã‚‹ï¼ˆã‚­ãƒ¼ã‚’è¿½åŠ ï¼‰
    query = st.text_input("ğŸ” Ask a question about the PDF:", key="pdf_query_input")
    
    if query:
        with st.spinner("ğŸ¤– Searching for the answer..."):
            try:
                retriever = qdrant.as_retriever()
                qa_chain = RetrievalQA.from_chain_type(
                    llm=select_model(), chain_type="stuff", retriever=retriever
                )
                response = qa_chain.run(query)

                # å›ç­”ã‚’è¡¨ç¤º
                st.markdown("### ğŸ“ Answer:")
                st.write(response if response else "No relevant information found in the PDF.")

            except Exception as e:
                st.error(f"âŒ Error during retrieval: {e}")





# **ãƒ¡ã‚¤ãƒ³é–¢æ•°**
def main():
    st.sidebar.header("ğŸ”§ Select a Tool")

    # Call select_model() only once
    llm = select_model()

    option = st.sidebar.radio("Select a function:", (
        "Chat Bot ğŸ¤–", "Website Summarizer ğŸŒ", "YouTube Summarizer ğŸ¥",
        "Image Recognition & Summary ğŸ–¼ï¸", "Speech Recognition ğŸ™ï¸",
        "Image Generation ğŸ¨", "PDF Upload & Vector Search ğŸ“„", "Ask My PDF ğŸ“œ"
    ), key="function_selection")
    
    if option == "Chat Bot ğŸ¤–":
        chat_bot(llm)
    elif option == "Website Summarizer ğŸŒ":
        website_summarizer(llm)
    elif option == "YouTube Summarizer ğŸ¥":
        youtube_summarizer(llm)
    elif option == "Image Recognition & Summary ğŸ–¼ï¸":
        image_recognition()  # No need to call select_model() inside
    elif option == "Speech Recognition ğŸ™ï¸":
        speech_recognition()
    elif option == "Image Generation ğŸ¨":
        image_generation()
    elif option == "PDF Upload & Vector Search ğŸ“„":
        pdf_upload_and_build_vector_db()
    elif option == "Ask My PDF ğŸ“œ":
        ask_my_pdf()

if __name__ == '__main__':
    main()
